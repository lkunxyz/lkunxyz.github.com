<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Tag: 内核 | Keen on Art of Tech]]></title>
  <link href="http://lkunxyz.github.com/tags/内核/atom.xml" rel="self"/>
  <link href="http://lkunxyz.github.com/"/>
  <updated>2013-01-01T20:55:27+08:00</updated>
  <id>http://lkunxyz.github.com/</id>
  <author>
    <name><![CDATA[Tiny]]></name>
    <email><![CDATA[admin@tinyxd.me]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[linux内存管理（下）]]></title>
    <link href="http://lkunxyz.github.com/blog/2012/07/31/linux-memory-management-3/"/>
    <updated>2012-07-31T23:51:00+08:00</updated>
    <id>http://lkunxyz.github.com/blog/2012/07/31/linux-memory-management-3</id>
    <content type="html"><![CDATA[<p><strong><em>摘要</em></strong>：把一块存放slab结构的内存区映射到一组连续的页框是最好的选择，这样会充分利用高速缓存并获得较低的平均访问时间。不过，上面的方式主要是针对那些使用非常频繁的内核数据结构——如task_struct、inode来设计的。如果对内存区的请求不是很频繁，那么，通过连续的线性地址，而不是物理地址来访问非连续的物理页框这样一种分配模式就会很有意义了。这种模式的主要优点是避免了外碎片，而缺点是必须打乱内核页表。此外，非连续内存区的大小必须是4096 的倍数。Linux 在几个方面使用非连续内存区：为活动的交换区分配数据结构，为模块分配空间，或者给某些I/O 驱动程序分配缓冲区等。此外，非连续内存区还提供了另一种使用高端内存页框的方法。</p>

<h2>非连续内存区的线性地址    </h2>

<p>要查找线性地址的一个空闲区，我们可以从PAGE_OFFSET开始查找（通常为0xc0000000，即第4 个GB 的起始地址）。下图让我们回忆了如何使用第4个GB 的线性地址：</p>

<!--more-->


<p>回忆一下：</p>

<iframe src="https://skydrive.live.com/embed?cid=1F260DE1061FCF3E&resid=1F260DE1061FCF3E%21156&authkey=ABbb5MTrh96mmAU" width="317" height="94" frameborder="0" scrolling="no"></iframe>


<p>（1）内存区的开始部分包含的是对前896MB RAM 进行映射的线性地址。直接映射的物理内存末尾所对应的线性地址保存在high_memory全局变量中。当物理内存小于896MB，则线性地址0xc0000000以后的896MB与其一一对应；当物理内存大于896MB而小于4GB时，只直接映射前896MB的地址到0xc0000000以后的线性空间，然后把线性空间的其他部分与896MB和4GB物理空间映射起来，称为动态重映射，这是本博的重点；当物理内存大于4GB，则需要考虑PAE的情况，其他的东东没什么区别，我们不做过多的回忆了。</p>

<p>（2）内核的页表由内核页全局目录变量swapper_pg_dir维护；pagetable_init()建立内核页表项。</p>

<p>（3）内存区的结尾部分包含的是固定映射的线性地址，主要用于存放一些常量线性地址，具体查看“高端内存映射”博文。</p>

<p>（4）从PKMAP_BASE 开始，我们查找用于高端内存页框的永久内核映射的线性地址，具体查看“高端内存映射 ”博文。</p>

<p>（5）其余的线性地址可以用于非连续内存区。在物理内存映射的末尾与第一个内存区之间插入一个大小为8MB（宏VMALLOC_OFFSET）的安全区，目的是为了“捕获”对内存的越界访问。出于同样的理由，插入其他4KB 大小的安全区来隔离非连续的内存区。</p>

<h2>非连续内存区的描述符 </h2>

<p>每个非连续内存区都对应着一个类型为vm_struct 的描述符：
```
struct vm_struct {</p>

<pre><code>void            *addr; 

unsigned long        size; 

unsigned long        flags; 

struct page        **pages; 

unsigned int        nr_pages; 

unsigned long        phys_addr; 

struct vm_struct    *next; 
</code></pre>

<p>};
```</p>

<p>介绍下它的字段：</p>

<p>void *    addr    内存区内第一个内存单元的线性地址（首址）</p>

<p>unsigned long    size    内存区的大小加4096（内存区之间的安全区间的大小）</p>

<p>unsigned long    flags    非连续内存区映射的内存的类型</p>

<p>struct page **    pages    指向nr_pages数组的指针，该数组由指向页描述符的指针组成</p>

<p>unsigned int    nr_pages    内存区填充的页的个数</p>

<p>unsigned long    phys_addr    该字段设为0，除非内存已被创建来映射一个硬件设备的I/O 共享内存</p>

<p>struct vm_struct *    next    指向下一个vm_struct结构的指针</p>

<h2>分配非连续内存区 </h2>

<p>vmalloc()函数给内核分配一个非连续内存区。参数size表示所请求内存区的大小。如果这个函数能够满足请求，就返回新内存区的起始地址；否则，返回一个NULL 指针（mm/ vmalloc.c）</p>

<p>其工作方式类似于kmalloc()，只不过vmalloc()分配的内存虚拟地址是连续的，而物理地址则无需连续。这也是用户空间分配函数的工作方式：由malloc()返回的页在进程的虚拟地址空间内是连续的，但是，这并不保证它们在物理RAM中也连续。kmalloc()函数确保页在物理地址上是连续的（虚拟地址自然也是连续的）。vmalloc()函数只确保页在虚拟地址空间内是连续的。vmalloc()仅在不得已时才会使用——一般是在为了获得大块内存时，例如，当模块被动态插入到内核中时，就把模块装载到由vmalloc()分配的内存上。</p>

<p>伙伴关系也好、slab技术也好，从内存管理理论角度而言目的基本是一致的，它们都是为了防止“分片”，不过分片又分为外部分片和内部分片之说，所谓内部分片是说系统为了满足一小段内存区（连续）的需要，不得不分配了一大区域连续内存给它，从而造成了空间浪费；外部分片是指系统虽有足够的内存，但却是分散的碎片，无法满足对大块“连续内存”的需求。无论何种分片都是系统有效利用内存的障碍。slab分配器使得一个页面内包含的众多小块内存可独立被分配使用，避免了内部分片，节约了空闲内存。伙伴关系把内存块按大小分组管理，一定程度上减轻了外部分片的危害，因为页框分配不在盲目，而是按照大小依次有序进行，不过伙伴关系只是减轻了外部分片，但并未彻底消除。你自己比划一下多次分配页面后，空闲内存的剩余情况吧。</p>

<p>所以避免外部分片的最终思路还是落到了如何利用不连续的内存块组合成“看起来很大的内存块”——这里的情况很类似于用户空间分配虚拟内存，内存逻辑上连续，其实映射到并不一定连续的物理内存上。Linux内核借用了这个技术，允许内核程序在内核地址空间中分配虚拟地址，同样也利用页表（内核页表）将虚拟地址映射到分散的内存页上。以此完美地解决了内核内存使用中的外部分片问题。内核提供vmalloc函数分配内核虚拟内存，该函数不同于kmalloc，它可以分配较Kmalloc大得多的内存空间（可远大于128K，但必须是页大小的倍数），但相比Kmalloc来说,Vmalloc需要对内核虚拟地址进行重映射，必须更新内核页表，因此分配效率上要低一些（用空间换时间）。</p>

<h2>释放函数   </h2>

<p>vfree()函数释放vmalloc()或vmalloc_32()创建的非连续内存区，而vunmap()函数释放vmap()创建的内存区。两个函数都使用同一个参数 —— 将要释放的内存区的起始线性地址address；它们都依赖于<code>__vunmap()</code>函数来做实质性的工作。</p>

<p><code>__vunmap()</code>函数接收两个参数：将要释放的内存区的起始地址的地址addr，以及标志deallocate_pages，如果被映射到内存区内的页框应当被释放到分区页框分配器（调用vfree()）中，那么这个标志被置位，否则被清除（vunmap()被调用）。该函数执行以下操作：</p>

<ol>
<li><p>调用remove_vm_area()函数得到vm_struct 描述符的地址area，并清除非连续内存区中的线性地址对应的内核的页表项。</p></li>
<li><p>如果deallocate_pages 被置位，函数扫描指向页描述符的area->pages指针数组；对于数组的每一个元素，调用<code>__free_page()</code>函数释放页框到分区页框分配器。此外，执行kfree(area->pages)来释放数组本身。</p></li>
<li><p>调用kfree(area)来释放vm_struct 描述符。</p></li>
</ol>


<br />


<p>本文章参考自《linux内核设计与实现》、<a href="http://blog.csdn.net/jiangyuping_fyl/article/details/7268287">slab分配器</a> 及 <a href="http://blog.csdn.net/yunsongice/article/details/5536197">非连续内存区</a> 。 <br/>
本站文章如果没有特别说明，均为<strong>原创</strong>，转载请以<strong>链接</strong>方式注明本文地址：<a href="http://tinyxd.me/blog/2012/07/31/linux-memory-management-3/">http://tinyxd.me/blog/2012/07/31/linux-memory-management-3/</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[linux内存管理（中）]]></title>
    <link href="http://lkunxyz.github.com/blog/2012/07/31/linux-memory-management-2/"/>
    <updated>2012-07-31T23:16:00+08:00</updated>
    <id>http://lkunxyz.github.com/blog/2012/07/31/linux-memory-management-2</id>
    <content type="html"><![CDATA[<p><strong><em>摘要</em></strong>：slab分配器是Linux内存管理中非常重要和复杂的一部分，其工作是针对一些经常分配并释放的对象，如进程描述符等，这些对象的大小一般比较小，如果直接采用伙伴系统来进行分配和释放，不仅会造成大量的内碎片，而且处理速度也太慢。而slab分配器是基于对象进行管理的，相同类型的对象归为一类(如进程描述符就是一类)，每当要申请这样一个对象，slab分配器就从一个slab列表中分配一个这样大小的单元出去，而当要释放时，将其重新保存在该列表中，而不是直接返回给伙伴系统。slab分配对象时，会使用最近释放的对象内存块，因此其驻留在CPU高速缓存的概率较高。</p>

<h2>简介</h2>

<!--more-->


<p>内存管理的目标是提供一种方法，为实现各种目的而在各个用户之间实现内存共享。内存管理方法应该实现以下两个功能：</p>

<ul>
<li><p>最小化管理内存所需的时间</p></li>
<li><p>最大化用于一般应用的可用内存（最小化管理开销）</p></li>
</ul>


<p>内存管理实际上是一种关于权衡的零和游戏。您可以开发一种使用少量内存进行管理的算法，但是要花费更多时间来管理可用内存。也可以开发一个算法来有效地管理内存，但却要使用更多的内存。最终，特定应用程序的需求将促使对这种权衡作出选择。</p>

<p>每个内存管理器都使用了一种基于堆的分配策略。在这种方法中，大块内存（称为 堆）用来为用户定义的目的提供内存。当用户需要一块内存时，就请求给自己分配一定大小的内存。堆管理器会查看可用内存的情况（使用特定算法）并返回一块内存。搜索过程中使用的一些算法有 first-fit（在堆中搜索到的第一个满足请求的内存块 ）和 best-fit（使用堆中满足请求的最合适的内存块）。当用户使用完内存后，就将内存返回给堆。</p>

<p>这种基于堆的分配策略的根本问题是碎片（fragmentation）。当内存块被分配后，它们会以不同的顺序在不同的时间返回。这样会在堆中留下一些洞，需要花一些时间才能有效地管理空闲内存。这种算法通常具有较高的内存使用效率（分配需要的内存），但是却需要花费更多时间来对堆进行管理。</p>

<p>另外一种方法称为 buddy memory allocation，是一种更快的内存分配技术，它将内存划分为 2 的幂次方个分区，并使用 best-fit 方法来分配内存请求。当用户释放内存时，就会检查 buddy 块，查看其相邻的内存块是否也已经被释放。如果是的话，将合并内存块以最小化内存碎片。这个算法的时间效率更高，但是由于使用 best-fit 方法的缘故，会产生内存浪费。</p>

<p>伙伴系统算法采用页框作为基本内存区，适合对大块内存的请求，为了解决外碎片的问题。而小内存的分配，会产生内碎片（internal fragmentation），这需要新的数据结构来描述在同一页框中 如何分配小内存区。</p>

<p>为了满足内核对这种小内存块的需要，Linux系统采用了一种被称为slab分配器的技术。Slab分配器的实现相当复杂，但原理不难，其核心思想就是“存储池[2]”的运用。内存片段（小块内存）被看作对象，当被使用完后，并不直接释放而是被缓存到“存储池”里，留做下次使用，这无疑避免了频繁创建与销毁对象所带来的额外负载。</p>

<p>Slab技术不但避免了内存内部分片（下文将解释）带来的不便（引入Slab分配器的主要目的是为了减少对伙伴系统分配算法的调用次数——频繁分配和回收必然会导致内存碎片——难以找到大块连续的可用内存），而且可以很好利用硬件缓存提高访问速度。</p>

<p>Slab并非是脱离伙伴关系而独立存在的一种内存分配方式，slab仍然是建立在页面基础之上，换句话说，Slab将页面（来自于伙伴关系管理的空闲页框链）撕碎成众多小内存块以供分配，slab中的对象分配和销毁使用kmem_cache_alloc与kmem_cache_free。每个高速缓存都是由kmem_cache_t(等价于struct kmem_cache)</p>

<p>slab分配器把对象分组放进高速缓存。包含高速缓存的主内存区被划分为多个slab，每个slab由一个或多个连续的页框组成，这些页框既包含已分配的对象，也包含空闲的对象。内核周期性地扫描高速缓存并释放空slab对应的页框。</p>

<p>普通高速缓存在系统初始化期间调用kmem_cache_init()和kmem_cache_sizes_init()来建立普通高速缓存。专用高速缓存由kmem_cache_create()函数创建。所有普通和专用高速缓存的名字都可以在运行期间通过读取/proc/slabinfo文件得到。这个文件也指明每个高速缓存中空闲对象的个数和已分配对象的个数。</p>

<p>slab的对象描述符与slab描述符本身类似，也可以用两种可能的方式来存放：外部对象描述符（存放在slab的外部）、内部对象描述符（存放在slab的内部）。每个对象都有类型为kmem_bufctl_t的一个描述符。</p>

<h2>API 函数 </h2>

<p>现在来看一下能够创建新 slab 缓存、向缓存中增加内存、销毁缓存的应用程序接口（API）以及 slab 中对对象进行分配和释放操作的函数。</p>

<p>第一个步骤是创建 slab 缓存结构，您可以将其静态创建为：</p>

<p>struct struct kmem_cache *my_cachep;</p>

<p>然后其他 slab 缓存函数将使用该引用进行创建、删除、分配等操作。kmem_cache 结构包含了每个中央处理器单元（CPU）的数据、一组可调整的（可以通过 proc 文件系统访问）参数、统计信息和管理 slab 缓存所必须的元素。</p>

<p>kmem_cache_create</p>

<p>内核函数 kmem_cache_create 用来创建一个新缓存。这通常是在内核初始化时执行的，或者在首次加载内核模块时执行。其原型定义如下：</p>

<p>struct kmem_cache * kmem_cache_create( const char <em>name, size_t size, size_t align,                        unsigned long flags;                        void (</em>ctor)(void<em>, struct kmem_cache </em>, unsigned long),                        void (<em>dtor)(void</em>, struct kmem_cache *, unsigned long));</p>

<p>name 参数定义了缓存名称，proc 文件系统（在 /proc/slabinfo 中）使用它标识这个缓存。 size 参数指定了为这个缓存创建的对象的大小， align 参数定义了每个对象必需的对齐。 flags 参数指定了为缓存启用的选项。这些标志如表 1 所示。</p>

<p>表 1. kmem_cache_create 的部分选项（在 flags 参数中指定）</p>

<p>选项                  说明</p>

<p>SLAB_RED_ZONE      在对象头、尾插入标志，用来支持对缓冲区溢出的检查。</p>

<p>SLAB_POISON            使用一种己知模式填充 slab，允许对缓存中的对象进行监视（对象属对象所有，不过可以在外部进行修改）。</p>

<p>SLAB_HWCACHE_ALIGN             指定缓存对象必须与硬件缓存行对齐。</p>

<p>ctor 和 dtor 参数定义了一个可选的对象构造器和析构器。构造器和析构器是用户提供的回调函数。当从缓存中分配新对象时，可以通过构造器进行初始化。</p>

<p>在创建缓存之后， kmem_cache_create 函数会返回对它的引用。注意这个函数并没有向缓存分配任何内存。相反，在试图从缓存（最初为空）分配对象时，refill 操作将内存分配给它。当所有对象都被使用掉时，也可以通过相同的操作向缓存添加内存。</p>

<p>kmem_cache_destroy</p>

<p>内核函数 kmem_cache_destroy 用来销毁缓存。这个调用是由内核模块在被卸载时执行的。在调用这个函数时，缓存必须为空。</p>

<p>void kmem_cache_destroy( struct kmem_cache *cachep );</p>

<p>kmem_cache_alloc</p>

<p>要从一个命名的缓存中分配一个对象，可以使用 kmem_cache_alloc 函数。调用者提供了从中分配对象的缓存以及一组标志：</p>

<p>void kmem_cache_alloc( struct kmem_cache *cachep, gfp_t flags );</p>

<p>这个函数从缓存中返回一个对象。注意如果缓存目前为空，那么这个函数就会调用 cache_alloc_refill 向缓存中增加内存。kmem_cache_alloc 的 flags 选项与 kmalloc 的 flags 选项相同。表 2 给出了标志选项的部分列表。</p>

<p>表 2. kmem_cache_alloc 和 kmalloc 内核函数的标志选项</p>

<p>标志    说明</p>

<p>GFP_USER 为用户分配内存（这个调用可能会睡眠）。</p>

<p>GFP_KERNEL       从内核 RAM 中分配内存（这个调用可能会睡眠）。</p>

<p>GFP_ATOMIC         使该调用强制处于非睡眠状态（对中断处理程序非常有用）。</p>

<p>GFP_HIGHUSER       从高端内存中分配内存。</p>

<p>kmem_cache_zalloc</p>

<p>内核函数 kmem_cache_zalloc 与 kmem_cache_alloc 类似，只不过它对对象执行 memset 操作，用来在将对象返回调用者之前对其进行清除操作。</p>

<p>kmem_cache_free</p>

<p>要将一个对象释放回 slab，可以使用 kmem_cache_free。调用者提供了缓存引用和要释放的对象。</p>

<p>void kmem_cache_free( struct kmem_cache <em>cachep, void </em>objp );</p>

<p>kmalloc 和 kfree</p>

<p>内核中最常用的内存管理函数是 kmalloc 和 kfree 函数。这两个函数的原型如下：</p>

<p>void <em>kmalloc( size_t size, int flags ); void kfree( const void </em>objp );</p>

<p>注意在 kmalloc 中，惟一两个参数是要分配的对象的大小和一组标志（请参看 表 2 中的部分列表）。但是 kmalloc 和 kfree 使用了类似于前面定义的函数的 slab 缓存。kmalloc 没有为要从中分配对象的某个 slab 缓存命名，而是循环遍历可用缓存来查找可以满足大小限制的缓存。找到之后，就（使用 <code>__kmem_cache_alloc</code>）分配一个对象。要使用 kfree 释放对象，从中分配对象的缓存可以通过调用 virt_to_cache 确定。这个函数会返回一个缓存引用，然后在 __cache_free 调用中使用该引用释放对象。</p>

<h2>其他函数 </h2>

<p>slab 缓存 API 还提供了其他一些非常有用的函数。 kmem_cache_size 函数会返回这个缓存所管理的对象的大小。您也可以通过调用kmem_cache_name 来检索给定缓存的名称（在创建缓存时定义）。缓存可以通过释放其中的空闲 slab 进行收缩。这可以通过调用kmem_cache_shrink 实现。注意这个操作（称为回收）是由内核定期自动执行的（通过 kswapd）。</p>

<p>unsigned int kmem_cache_size( struct kmem_cache <em>cachep ); const char </em>kmem_cache_name( struct kmem_cache <em>cachep ); int kmem_cache_shrink( struct kmem_cache </em>cachep );</p>

<h2>补充： </h2>

<p>如果对存储区的请求不频繁，就用一组普通高速缓存来处理，普通高速缓存中的对象具有几何分布的大小，范围为32~131072字节。</p>

<p>使用kmalloc()函数申请，kfree()释放。</p>

<br />


<p>本文章参考自《深入理解linux内核》及 <a href="https://www.ibm.com/developerworks/cn/linux/l-linux-slab-allocator/">动态内存管理</a>。 <br/>
本站文章如果没有特别说明，均为<strong>原创</strong>，转载请以<strong>链接</strong>方式注明本文地址：<a href="http://tinyxd.me/blog/2012/07/31/linux-memory-management-2/">http://tinyxd.me/blog/2012/07/31/linux-memory-management-2/</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[linux内存管理（上）]]></title>
    <link href="http://lkunxyz.github.com/blog/2012/07/29/linux-memory-management-1/"/>
    <updated>2012-07-29T23:33:00+08:00</updated>
    <id>http://lkunxyz.github.com/blog/2012/07/29/linux-memory-management-1</id>
    <content type="html"><![CDATA[<p><strong><em>摘要</em></strong>：内核如何给自己分配动态内存呢？</p>

<p>“页框管理”和“内存区管理”是对连续物理内存区处理的两种不同技术。</p>

<p>“非连续内存区管理”是处理非连续内存区的第三种技术。</p>

<h2>页框管理  </h2>

<p>intel的Pentium处理器可以采用两种不同的页框大小：4KB和4MB。Linux采用4KB页框大小作为标准的内存分配单元。物理页在系统中由页框结构struct paga描述，系统中所有的页框存储在数组mem_map[]中，可以通过该数组找到系统中的每一页（空闲或非空闲）。</p>

<!--more-->


<h2>内存管理区  </h2>

<p>Linux2.6把每个内存节点的物理内存划分为3个管理区（zone）。在80X86UMA体系结构中的管理区为：</p>

<p>ZONE_DMA：包含低于16MB的内存页框</p>

<p>ZONE_NORMAL：包含高于16MB且低于896MB的内存页框</p>

<p>ZONE_HIGHMEM：包含从896MB开始高于896MB的内存页框（并不映射在内核线性地址空间的第4个GB）</p>

<p>注意：ZONE_DMA和ZONE_NORMAL区包含内存的“常规”页框，通过把它们线性映射到线性地址空间的第4个GB，内核就可以直接进行访问。相反，ZONE_HIGHMEM区包含的内存页不能由内核直接访问，尽管它们也线性地映射到了线性地址空间的第4个GB。在64位体系结构上ZONE_HIGHMEM区总是空的。</p>

<p>Linux内核管理物理内存是通过分页机制实现的，它将整个内存划分成无数4k(在i386体系结构中)大小页，从而分配和回收内存的基本单位便是内存页了。利用分页管理有助于灵活分配内存地址，因为分配时不必要求必须有大块的连续内存，系统可以东一页、西一页的凑出所需要的内存供进程使用。虽然如此，但是实际上系统使用内存还是倾向于分配连续的内存块，因为分配连续内存时，页表不需要更改，因此能降低TLB的刷新率（频繁刷新会很大增加访问速度）。</p>

<p>鉴于上述需求，内核分配物理页为了尽量减少不连续情况，采用了“伙伴”关系来管理空闲页框。伙伴关系分配算法大家不应陌生——几乎所有操作系统书都会提到,我们不去详细说它了，如果不明白可以参看有关资料。这里只需要大家明白Linux中空闲页面的组织和管理利用了伙伴关系，因此空闲页面分配时也需要遵循伙伴关系，最小单位只能是2的幂倍页面大小。内核中分配空闲页框的基本函数是get_free_page/get_free_pages，它们或是分配单页或是分配指定的页框（2、4、8…512页）。</p>

<p>注意：get_free_page是在内核中分配内存，不同于malloc在用户空间中分配，malloc利用堆动态分配，实际上是调用brk()系统调用，该调用的作用是扩大或缩小进程堆空间（它会修改进程的brk域）。如果现有的内存区域不够容纳堆空间，则会以页面大小的倍数位单位，扩张或收缩对应的内存区域，但brk值并非以页面大小为倍数修改，而是按实际请求修改。因此Malloc在用户空间分配内存可以以字节为单位分配,但内核在内部仍然会是以页为单位分配的。</p>

<h2>高端内存页框的内核映射  </h2>

<p>内核可以采用三种不同的机制将页框映射到高端内存；分别叫做永久内核映射、临时内核映射及非连续内存分配。</p>

<p>永久内核映射可能阻塞当前进程，不能用于中断处理程序和可延迟函数。</p>

<p>临时内核映射比永久内核映射的实现要简单，可以用在中断处理程序和可延迟函数的内部，因为它们从不阻塞当前进程。</p>

<h2>伙伴系统算法（buddy systerm）  </h2>

<p>内核要分配一组连续的页框，必须建立一种健壮、高效的分配策略。为此，必须解决著名的外部碎片（external fragmentation）问题。频繁地请求和释放不同大小的一组连续页框，必然导致在已分配页框的块内分散了许多小块的空闲页框。由此带来的问题是，即使有足够的空闲页框可以满足请求，但要分配一个大块的连续页框就可能无法满足。</p>

<p>Linux 采用伙伴系统（buddy system）算法来解决外碎片问题。把所有的空闲页框分组为11个块链表，每个块链表分别包含大小为1, 2, 4, 8, 16, 32, 64, 128, 256，512和1024 个连续的页框。对1024 个页框的最大请求对应着4MB 大小的连续RAM块。每个块的第一个页框的物理地址是该块大小的整数倍。例如，大小为16 个页框的块，其起始地址是16 × 212（212 ＝ 4096，这是一个常规页的大小）的倍数。</p>

<p>本文章参考自《深入理解linux内核》及 <a href="http://www.cnblogs.com/hoys/archive/2011/09/08/2171606.html">Linux内存管理(上)</a> 。 <br/>
本站文章如果没有特别说明，均为<strong>原创</strong>，转载请以<strong>链接</strong>方式注明本文地址：<a href="http://tinyxd.me/blog/2012/07/29/linux-memory-management-1/">http://tinyxd.me/blog/2012/07/29/linux-memory-management-1/</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[linux进程调度]]></title>
    <link href="http://lkunxyz.github.com/blog/2012/07/26/linux-process-scheduling/"/>
    <updated>2012-07-26T23:18:00+08:00</updated>
    <id>http://lkunxyz.github.com/blog/2012/07/26/linux-process-scheduling</id>
    <content type="html"><![CDATA[<p><strong><em>摘要</em></strong>：linux与任何分时系统一样，通过一个进程到另一个进程的快速切换，达到表面上看来的多个进程同时执行的神奇效果。</p>

<br />


<p>Linux的调度基于分时（time sharing）技术：多个进程以“时间多路复用”方式运行，因为CPU的时间被分成“片（slice）”，给每个可运行进程分配一片。</p>

<p>在linux中，进程的优先级是动态的。调度程序跟踪进程正在做什么，并周期性地调整它们的优先级。</p>

<h2>分类 </h2>

<p>传统上，把进程分类为“IO受限（IO bound）”或“CPU受限（CPU bound）”。前者频繁使用IO设备，并花费很多时间等待IO操作的完成；而后者则需要大量CPU时间的数值计算应用程序。</p>

<p>另一种分类方法把进程区分为三类：交互式进程（interactive process）、批处理进程（batch process、实时进程（real-time process）。</p>

<!--more-->


<p></p>

<h2>进程的抢占 </h2>

<p>Linux的进程是抢占式的。根据优先级和时间片是否国企决定是否可以被抢占。</p>

<p>Linux2.6内核是抢占式的，这意味着进程无论是处于内核态还是用户态，都可能被抢占。</p>

<h2>调度算法 </h2>

<p>调度程序总能成功地找到要执行的进程。事实上，总是至少有一个可运行进程，即swapper进程，它的PID等于0，而且它只有在CPU不能执行其他进程时才执行。</p>

<p>每个Linux进程总是按照下面的调度类型被调度：</p>

<p>SCHED_FIFO（先进先出的实时进程）：直到先被执行的进程变为非可执行状态，后来的进程才被调度执行。在这种策略下，先来的进程可以执行sched_yield系统调用，自愿放弃CPU，以让权给后来的进程；</p>

<p>SCHED_RR（时间片轮转的实时进程）：轮转调度。内核为实时进程分配时间片，在时间片用完时，让下一个进程使用CPU；</p>

<p>SCHED_NORMAL（普通的分时进程）。</p>

<h2>普通进程的调度 (参考自<a href="http://hi.baidu.com/_kouu/blog/item/52471ab5e90e7c788ad4b24a.html">linux进程调度浅析</a>) </h2>

<p>实时进程调度的中心思想是，让处于可执行状态的最高优先级的实时进程尽可能地占有CPU，因为它有实时需求；而普通进程则被认为是没有实时需求的进程，于是调度程序力图让各个处于可执行状态的普通进程和平共处地分享CPU，从而让用户觉得这些进程是同时运行的。</p>

<p>每个普通进程都有它自己的静态优先级，还有动态优先级。</p>

<p>与实时进程相比，普通进程的调度要复杂得多。内核需要考虑两件麻烦事：</p>

<p>一、动态调整进程的优先级</p>

<p>按进程的行为特征，可以将进程分为“交互式进程”和“批处理进程”：</p>

<p>交互式进程（如桌面程序、服务器、等）主要的任务是与外界交互。这样的进程应该具有较高的优先级，它们总是睡眠等待外界的输入。而在输入到来，内核将其唤醒时，它们又应该很快被调度执行，以做出响应。比如一个桌面程序，如果鼠标点击后半秒种还没反应，用户就会感觉系统“卡”了；</p>

<p>批处理进程（如编译程序）主要的任务是做持续的运算，因而它们会持续处于可执行状态。这样的进程一般不需要高优先级，比如编译程序多运行了几秒种，用户多半不会太在意；</p>

<p>如果用户能够明确知道进程应该有怎样的优先级，可以通过nice、setpriority系统调用来对优先级进行设置。（如果要提高进程的优先级，要求用户进程具有CAP_SYS_NICE能力。）</p>

<p>然而应用程序未必就像桌面程序、编译程序这样典型。程序的行为可能五花八门，可能一会儿像交互式进程，一会儿又像批处理进程。以致于用户难以给它设置一个合适的优先级。</p>

<p>再者，即使用户明确知道一个进程是交互式还是批处理，也多半碍于权限或因为偷懒而不去设置进程的优先级。（你又是否为某个程序设置过优先级呢？）</p>

<p>于是，最终，区分交互式进程和批处理进程的重任就落到了内核的调度程序上。</p>

<p>调度程序关注进程近一段时间内的表现（主要是检查其睡眠时间和运行时间），根据一些经验性的公式，判断它现在是交互式的还是批处理的？程度如何？最后决定给它的优先级做一定的调整。</p>

<p>进程的优先级被动态调整后，就出现了两个优先级：</p>

<p>1、用户程序设置的优先级（如果未设置，则使用默认值），称为静态优先级。这是进程优先级的基准，在进程执行的过程中往往是不改变的；</p>

<p>2、优先级动态调整后，实际生效的优先级。这个值是可能时时刻刻都在变化的；</p>

<p>二、调度的公平性</p>

<p>在支持多进程的系统中，理想情况下，各个进程应该是根据其优先级公平地占有CPU。而不会出现“谁运气好谁占得多”这样的不可控的情况。</p>

<p>linux实现公平调度基本上是两种思路：</p>

<p>1、给处于可执行状态的进程分配时间片（按照优先级），用完时间片的进程被放到“过期队列”中。等可执行状态的进程都过期了，再重新分配时间片；</p>

<p>2、动态调整进程的优先级。随着进程在CPU上运行，其优先级被不断调低，以便其他优先级较低的进程得到运行机会；</p>

<p>后一种方式有更小的调度粒度，并且将“公平性”与“动态调整优先级”两件事情合而为一，大大简化了内核调度程序的代码。因此，这种方式也成为内核调度程序的新宠。</p>

<p>强调一下，以上两点都是仅针对普通进程的。而对于实时进程，内核既不能自作多情地去动态调整优先级，也没有什么公平性可言。</p>

<p>普通进程具体的调度算法非常复杂，并且随linux内核版本的演变也在不断更替（不仅仅是简单的调整），所以本文就不继续深入了。有兴趣的朋友可以参考下面的链接：</p>

<p><a href="http://www.ibm.com/developerworks/cn/linux/l-cn-scheduler/">《Linux 调度器发展简述》</a></p>

<p><a href="http://blog.chinaunix.net/u1/42957/showart.php?id=337597">《鼠眼看Linux调度器》</a></p>

<p><a href="http://blog.chinaunix.net/u1/42957/showart.php?id=337604">《鼠眼再看Linux调度器［1］》</a></p>

<p><a href="http://blog.chinaunix.net/u1/42957/showart.php?id=337607">《鼠眼再看Linux调度器［2］》</a></p>

<h2>实时进程的调度 </h2>

<p>每个实时进程都与一个实时优先级相关，实时优先级是一个范围从1（最高优先级）~99（最低优先级）的值。调度程序总是让优先级高的进程运行，换句话说，实时进程运行的过程中，禁止低优先级的进程的执行。与普通进程相反，实时进程总是被当成活动进程。用户可以通过系统调用sched_setparam()和sched_setscheduler()改变进程的实时优先级。</p>

<p>只有在下述时间之一发生时，实时进程才会被另一个进程取代：</p>

<pre><code>进程被另外一个具有更高实时优先级的实时进程抢占 

进程执行了阻塞操作并进入睡眠（处于TASK_INTERRUPTIBLE或TASK_UNINTERRUPTIBLE状态）。 

进程停止（处于TASK_STOPPED或TASK_TRACED状态）或被杀死（处于EXIT_ZOMBIE或EXIT_DEAD状态）。 

进程通过调用系统调用sched_yield()自愿放弃CPU。 

进程是基于时间片轮转的实时进程（SCHED_RR），而且用完了它的时间片。 
</code></pre>

<p>数据结构runqueue是Linux2.6调度程序最重要的数据结构。系统中的每个CPU都有它自己的运行队列，所有的runqueue结构存放在runqueues每CPU变量中。</p>

<h2>调度程序所使用的函数 </h2>

<p>scheduler_tick()：维持当前最新的time_slice计数器</p>

<p>try_to_wake_up()：唤醒睡眠进程</p>

<p>recalc_task_prio()：更新进程的动态优先级</p>

<p>schedule()：选择要被执行的新进程</p>

<p>load_balance()：维持多处理器系统中运行队列的平衡</p>

<h2>多处理器系统中运行队列的平衡 </h2>

<p>Linux一直坚持采用对称多处理模式，这意味着，与其他CPU相比，内核不对一个CPU有任何偏向，但是，多处理器机器具有很多不同的风格，而且调度程序的实现随硬件特征的不同而有所不同，我们将特别关注下面三种不同类型的多处理器机器：</p>

<p>（1）标准的多处理器体系结构</p>

<p>直到最近，这是多处理器机器最普通的体系结构。这些机器所共有的RAM芯片集被所有CPU共享。</p>

<p>（2）超线程</p>

<p>超线程芯片是一个立刻执行几个执行线程的微处理器；它包括几个内部寄存器的拷贝，并快速在它们之间切换。这种由Intel发明的技术，使得当前线程在访问内存的间隙，处理器可以使用它的机器周期去执行另外一个线程。一个超线程的物理CPU可以被Linux看作几个不同的逻辑CPU。</p>

<p>（3）NUMA</p>

<p>把CPU和RAM以本地“结点”为单位分组，（通常一个结点包括一个CPU和几个RAM芯片）。内存仲裁器（一个使系统中的CPU以串型方式访问RAM的专用电路）是典型的多处理器系统的瓶颈。在NUMA体系结构中，当CPU访问与它同在一个结点中的“本地”RAM芯片时，几乎没有竞争，因此访问通常是非常快的。另一方面，访问它所属结点外的“远程”RAM芯片就非常慢。</p>

<h2>与调度相关的系统调用 </h2>

<p>nice()、getpriority()和setpriority()系统调用、sched_getaffinity()和sched_setaffinity()调用</p>

<h2>与实时进程相关的系统调用 </h2>

<p>sched_getscheduler()和sched_setscheduler()系统调用、sched_getparam()和sched_setparam()系统调用、sched_yield()系统调用、sched_get_priority_min()和 sched_get_priority_max()系统调用、sched_rr_get_interval()系统调用</p>

<br />


<p>本文章参考自《深入理解linux内核》。 <br/>
本站文章如果没有特别说明，均为<strong>原创</strong>，转载请以<strong>链接</strong>方式注明本文地址：<a href="http://tinyxd.me/blog/2012/07/26/linux-process-scheduling/">http://tinyxd.me/blog/2012/07/26/linux-process-scheduling/</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[linux系统定时测量]]></title>
    <link href="http://lkunxyz.github.com/blog/2012/07/26/linux-timing-measurements/"/>
    <updated>2012-07-26T12:48:00+08:00</updated>
    <id>http://lkunxyz.github.com/blog/2012/07/26/linux-timing-measurements</id>
    <content type="html"><![CDATA[<p>Linux内核必须完成的两种主要定时测量：</p>

<p>保存当前的时间和日期，以便通过time()、ftime()、gettimeofday()系统调用把它们返回给用户程序，也可以由内核本身把当前时间作为文件和网络包的时间戳。</p>

<p>维持定时器，这种机制能够告诉内核或用户程序某一时间间隔已经过去了。</p>

<p>一般会遇到的几种时钟和定时器电路：实时时钟（Real Time Clock RTC）、时间戳计数器（Time Stamp Counter TSC）、可编程间隔定时器（Programmable Interval Timer PIT）、CPU本地定时器（APIC中）、高精度事件定时器（HPET）、ACPI电源管理定时器。</p>

<p>Linux的计时体系结构是一组与时间流相关的内核数据结构和函数。实际上，基于80x86多处理器机器所具有的计时体系结构与单处理器机器所具有的稍有不同：</p>

<p>在三处理器系统上，所有的计时活动都是由全局定时器（可以是可编程间隔定时器也可以是高精度事件定时器）产生的中断触发的。</p>

<!--more-->


<p>在多处理器系统上，所有普通的活动（像软定时器处理）都是由全局定时器产生的中断触发的，而具体的CPU的活动（像监控当前运行进程的执行时间）是由本地APIC定时器产生的中断触发的。</p>

<p>jiffies</p>

<p>jiffies是一个计数器，用来记录自系统启动依赖产生的节拍总数。启动时，内核将该变量初始化为0，此后，每次时钟中断处理程序都会增加该变量的值。因为一秒内时钟中断的次数等于Hz，所以jiffies一秒内增加的值也就为Hz。系统运行时间以秒为单位计算，就等于jiffies/Hz。</p>

<p>xtime</p>

<p>xtime变量存放当前时间和日期；它是一个timespec类型的数据结构，该结构有两个字段：</p>

<p>1.tv_sec 存放自1970年1月1日（UTC）午夜以来经过的秒数。</p>

<p>2.tv_nsec存放自上一秒开始经过的纳秒数（它的值域范围在0-999999999之间）</p>

<p>在单处理器系统上，所有与定时有关的活动都是由IRQ线0上的可编程间隔定时器产生的中断触发的。</p>

<p>多处理器系统可以依赖两种不同的时钟中断源：可编程间隔定时器或高精度事件定时器产生的中断，以及CPU本地定时器产生的中断(监管内核代码并检测当前进程在特定CPU上已经运行了多长时间)。</p>

<p>内核在于定时相关的其他任务中必须周期性地收集若干数据用于：</p>

<pre><code>检查运行进程的CPU资源限制  

更新与本地CPU工作负载有关的统计数  

计算平均系统负载  

监管内核代码  
</code></pre>

<p>软定时器和延迟函数</p>

<p>定时器是一种软件功能，即允许在将来的某个时期，函数在给定的时间间隔用完时被调用。超时（time-out）表示与定时器相关的时间间隔已经用完的那个时刻。</p>

<p>Linux考虑两种类型的定时器，即动态定时器（dynamic timer）和间隔定时器（interval timer）。第一种类型由内核使用，而间隔定时器可以由进程在用户态创建。</p>

<p>延迟函数</p>

<p>当内核需要等待一个较短的时间间隔（比方说，不超过几毫秒）时，就不需要使用软定时器。</p>

<p>开发驱动，需要较短的时间间隔，在以上情况下，内核使用udelay()和ndelay()函数：前者接收一个微妙级的时间间隔作为它的参数，并在指定的延迟结束后返回；后者与前者类似，但是指定的延迟参数是纳秒级的。</p>

<p>本文章参考自《深入理解linux内核》。 <br/>
本站文章如果没有特别说明，均为<strong>原创</strong>，转载请以<strong>链接</strong>方式注明本文地址：<a href="http://tinyxd.me/blog/2012/07/26/linux-timing-measurements/">http://tinyxd.me/blog/2012/07/26/linux-timing-measurements/</a></p>
]]></content>
  </entry>
  
</feed>
